{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Spam Mail Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1760k  100 1760k    0     0   118k      0  0:00:14  0:00:14 --:--:--  228k\n"
     ]
    }
   ],
   "source": [
    "! curl http://nlp.cs.aueb.gr/software_and_datasets/Enron-Spam/preprocessed/enron1.tar.gz --output enron1.tar.gz\n",
    "! tar -xf enron1.tar.gz enron1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    3672\n"
     ]
    }
   ],
   "source": [
    "! ls -1 enron1/ham/*.txt | wc -l # print the number of non-spam emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    1500\n"
     ]
    }
   ],
   "source": [
    "! ls -1 enron1/spam/*.txt | wc -l # print the number of spam emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: mcmullen gas for 11 / 99\n",
      "jackie ,\n",
      "since the inlet to 3 river plant is shut in on 10 / 19 / 99 ( the last day of\n",
      "flow ) :\n",
      "at what meter is the mcmullen gas being diverted to ?\n",
      "at what meter is hpl buying the residue gas ? ( this is the gas from teco ,\n",
      "vastar , vintage , tejones , and swift )\n",
      "i still see active deals at meter 3405 in path manager for teco , vastar ,\n",
      "vintage , tejones , and swift\n",
      "i also see gas scheduled in pops at meter 3404 and 3405 .\n",
      "please advice . we need to resolve this as soon as possible so settlement\n",
      "can send out payments .\n",
      "thanks"
     ]
    }
   ],
   "source": [
    "! cat enron1/ham/0007.1999-12-14.farmer.ham.txt # print an example of non-spam (ham) email"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement ntlk (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for ntlk\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 22.0.3 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "/System/Library/Frameworks/Python.framework/Versions/2.7/Resources/Python.app/Contents/MacOS/Python: No module named nltk\n"
     ]
    }
   ],
   "source": [
    "! pip3 install nltk\n",
    "! python -m nltk.downloader all #이 부분에서 많은 시간을 소요합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os\n",
    "\n",
    "# init\n",
    "\"\"\"\n",
    "emails: a set of email\n",
    "labels: a set of label representing whetere the gien email is spam or ham\n",
    "  - spam: 1\n",
    "  - ham: 0\n",
    "\"\"\"\n",
    "\n",
    "emails, labels = [], []\n",
    "parition = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of emails = 5172\n",
      "# of labels = 5172\n"
     ]
    }
   ],
   "source": [
    "# load spam dataset\n",
    "file_path = 'enron1/spam'\n",
    "\n",
    "for fname in glob.glob(os.path.join(file_path, '*.txt')):\n",
    "    with open(fname, 'r', encoding='ISO-8859-1') as f: # [!important] check encofing format\n",
    "      emails.append(f.read())\n",
    "      labels.append(1)\n",
    "\n",
    "file_path = 'enron1/ham'\n",
    "for fname in glob.glob(os.path.join(file_path, '*.txt')):\n",
    "    with open(fname, 'r', encoding='ISO-8859-1') as f: \n",
    "      emails.append(f.read())\n",
    "      labels.append(0) \n",
    "\n",
    "print('# of emails = {}\\n# of labels = {}'.format(len(emails), len(labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "  - remove number and punctuation\n",
    "  - remove name entity\n",
    "  - remove stopword\n",
    "  - lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove number and punctuation \n",
    "def letters_only(word):\n",
    "    return word.isalpha()\n",
    "\n",
    "# remove name entity\n",
    "from nltk.corpus import names\n",
    "all_names = set(names.words())\n",
    "\n",
    "# lemmaization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "# put all together to clean texts\n",
    "def clean_text(doc):\n",
    "    cleaned_doc = []\n",
    "    for word in doc.split(' '): # split doc. by blank (' ')\n",
    "        word = word.lower() # ABD -> abd\n",
    "        if letters_only(word) and word not in all_names and len(word) > 2: # remove number and punc. and name entity\n",
    "            cleaned_doc.append(lemmatizer.lemmatize(word))\n",
    "            \n",
    "    return ' '.join(cleaned_doc) \n",
    "\n",
    "cleaned_emails = [clean_text(doc) for doc in emails]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "* Split data into train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(cleaned_emails, labels, test_size=0.33, random_state=486)\n",
    "\n",
    "cv = CountVectorizer(stop_words='english', max_features=500)\n",
    "term_docs_train = cv.fit_transform(X_train) # get counter vector for X_train\n",
    "term_docs_test = cv.transform(X_test) # get counter vector for X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Model and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gojunseo/Library/Python/3.8/lib/python/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/Users/gojunseo/Library/Python/3.8/lib/python/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "gnb = MultinomialNB()\n",
    "gnb.fit(term_docs_train.todense(), Y_train)\n",
    "y_pred = gnb.predict(term_docs_test.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model is: 0.90\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "acc = accuracy_score(Y_test, y_pred)\n",
    "print(\"Accuracy of the model is: {:.2f}\".format(acc))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
